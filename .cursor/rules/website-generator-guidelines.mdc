---
description: "Project guidance and development practices for Docker-first application development"
globs: ["**/*"]
alwaysApply: true
---

# Docker-First Project Guidelines

> **üìã About This Document:** This is the Cursor project rule that automatically gets added as context to every AI prompt in this workspace in Cursor (YOU). It provides essential guidance about the project's architecture, development practices, and constraints that must be followed when generating or modifying code.

## üõ°Ô∏è Project Integrity & Compliance

**üö® CRITICAL COMPLIANCE RULES for AI Assistants:**

### Pre-Implementation Verification:
**BEFORE building or modifying anything, if a user request would deviate from the established architecture, patterns, or guidelines defined in this capstone project:**

1. **STOP immediately** and ask the user:
   > "Are you sure you want to build this in a way that's not in line with what was defined for this capstone project in this file? This could create issues with the established architecture."

2. **Wait for explicit user confirmation** before proceeding with any non-compliant changes

### Post-Implementation Verification:
**AFTER building or modifying anything:**

1. **Self-check compliance** by asking:
   - "Is what I just built aligned with this project's defined architecture?"
   - "Does this follow the three-version parallel generation approach?"
   - "Is this consistent with the Docker-first methodology?"
   - "Does this maintain the established patterns and constraints?"

2. **If misalignment is detected:**
   - **Immediately alert the user** about the compliance issue
   - **Suggest reverting the changes** to maintain project integrity
   - **Explain potential issues** that could arise from the deviation

### Established Project Patterns:
- **Three-Version AI Generation** - AI generates 3 creative variations of each website in parallel
- **Two-Stage AI Process** - GPT-4-turbo generates creative directions, GPT-5.1 generates HTML for each version
- **Docker-First Architecture** - All services containerized, no local dependencies
- **FastAPI Backend** - Python-based API with async processing and database persistence
- **Single-Command Deployment** - Maintain zero-configuration setup approach
- **Professional Variety** - Three distinct design approaches: Color Palette, Design System, and Layout variations

**üéØ Goal:** Preserve the integrity of this capstone project's carefully designed architecture and prevent deviations that could undermine the established three-version parallel generation approach.

## üöÄ Single-Command Docker Setup

This project is designed for **zero-configuration deployment**. Everything works out of the box with a single command.

### üéØ Core Architecture:
- **All services run in Docker containers** - no local runtime dependencies needed
- **All dependencies managed inside containers** - package installation happens in Docker
- **No lock files in git** - Docker generates dependencies fresh every time
- **Servers bind to `0.0.0.0`** - proper container networking
- **Applications optimized for Docker** - polling enabled, hot reload configured appropriately

### üöÄ First-Time Setup:

**Clone and run - that's it:**
```bash
git clone <repo-url>
cd <project-name>
docker-compose up --build -d
```

No additional setup, no dependency management, no configuration needed.

### üîÑ Development Workflow:

```bash
# First, ensure Docker daemon is running:
docker info > /dev/null 2>&1 || { echo "Docker daemon not running. Please start Docker Desktop."; exit 1; }

# When asked to "run the project":
docker-compose up --build -d

# Check readiness with health checks:
curl -s http://localhost:8000/health

# Automatically open browser:
open http://localhost:8000  # macOS
# or xdg-open http://localhost:8000  # Linux
```

**Why this workflow:**
- ‚úÖ **Docker daemon check first** - prevents cryptic errors if Docker daemon isn't running
- ‚úÖ **Terminal returns control** - can continue with next steps
- ‚úÖ **Programmatic health checks** - verify readiness
- ‚úÖ **Automatic browser opening** - seamless user experience
- ‚úÖ **Clean task completion** - clear start/finish workflow

### üì¶ Adding Dependencies:
```bash
# Add dependencies inside containers:
docker-compose run --rm --no-deps <service-name> <package-manager> install <package>

# Then rebuild and restart:
docker-compose up --build -d
```

### üéØ What's Gitignored (Docker-Managed):
- `node_modules/` / `venv/` / similar - Generated inside containers
- Package lock files - Generated fresh on each build
- Build artifacts - Dynamically generated files
- Temporary files - Runtime-generated content

### üéØ Design Philosophy:

**"Clone and Run"** - No local setup, no dependency hell, no configuration. Just Docker.

### üìñ Documentation Policy:

**This project has NO README file** - and none should be created.

**Why no README:**
- ‚úÖ **This project guidelines file IS the complete documentation**
- ‚úÖ **Automatically accessible in Cursor** - always available as context
- ‚úÖ **Self-maintaining** - updates with the project through AI assistance
- ‚úÖ **Rich formatting** - better than traditional README files
- ‚úÖ **Always current** - living document that evolves with the codebase

**üö® ABSOLUTE RULE for AI Assistants:**
- **NEVER create README.md, readme.txt, or any standalone documentation files**
- **ALL documentation belongs in this project guidelines file**
- **Refer users to this file for ALL project information**

### üîë Environment Configuration Rules

**CRITICAL:** Environment file handling follows strict security protocols:

- **`env.template`** - Template file showing required environment variables (safe for git, Cursor accessible)
- **`.env`** - Actual environment file with real secrets (gitignored, Cursor-ignored, user-managed)

**üö® ABSOLUTE RULES for AI Assistants:**
1. **NEVER create or recreate the `.env` file** - it exists and is user-managed
2. **TRUST that `.env` exists** - even though Cursor can't see it due to security blocking
3. **ASSUME `.env` contains the same structure as `env.template`** but with actual values filled in
4. **ONLY modify `env.template`** when environment variable changes are needed
5. **ALWAYS refer users to copy `env.template` to `.env`** for initial setup

**Why this setup:**
- ‚úÖ **Security:** Real secrets never visible to AI or git
- ‚úÖ **Functionality:** Docker and applications can access `.env` normally  
- ‚úÖ **Template:** `env.template` shows structure without exposing secrets
- ‚úÖ **Standard:** Follows industry-standard environment file practices

## üåê Website Generator Application

Full-stack application that scrapes websites and generates three creative variations using AI-powered parallel generation.

### Architecture:
- **Frontend**: Clean web interface for URL input, version switching, and result viewing
- **Backend**: FastAPI service handling scraping, AI generation coordination, and storage
- **Database**: PostgreSQL instance for persistence, job tracking, and multi-version storage
- **AI Processing**: Two-stage OpenAI GPT integration with parallel execution
  - **Stage 1 - Creative Directions**: GPT-4-turbo generates 3 distinct design approaches (Color Palette, Design System, Layout)
  - **Stage 2 - Parallel Generation**: GPT-5.1 generates 3 complete HTML websites simultaneously following each direction

### Usage:
```bash
# Start the application:
docker-compose up --build -d

# Access web interface:
open http://localhost:8000

# Check system health:
curl -s http://localhost:8000/health
```

### Core Features:
1. **Web Interface**: Responsive UI with URL input, version switching controls, and preview gallery
2. **Async Processing**: Background job queue with progress tracking and parallel generation
3. **Three-Version Generation**: Creates 3 distinct design variations (Color, Design System, Layout) for each website
4. **Parallel Execution**: All 3 versions generated simultaneously using async processing
5. **AI Creative Directions**: GPT-4-turbo analyzes content and generates unique design philosophies for each version
6. **Persistent Storage**: Database persistence with multi-version tracking for each website
7. **Smart Identifiers**: Intelligent URL-to-identifier extraction
8. **Secure Viewing**: Sandboxed iframe display with version switching capabilities

### Three-Version Generation System:
Two-stage AI process with parallel execution for creating design variations:

**Stage 1 - Creative Direction Generator (GPT-4-turbo):**
- Analyzes scraped website content (title, description, text, images)
- Generates 3 distinct creative directions as JSON: `version_1`, `version_2`, `version_3`
- Each direction specifies a unique design philosophy:
  - **Version 1 - Color Palette Innovation**: Bold color scheme transformations
  - **Version 2 - Design System Shift**: Typography, spacing, and visual hierarchy variations
  - **Version 3 - Layout Revolution**: Alternative page structures and information architecture
- Returns detailed instructions (3-5 paragraphs each) for implementing each creative direction

**Stage 2 - Parallel HTML Generation (GPT-5.1):**
- Receives creative instructions for each version plus scraped content
- Generates 3 complete websites in parallel using asyncio
- Uses GPT-5.1 Responses API with high reasoning effort
- Outputs production-ready HTML with Tailwind CSS styling
- All 3 generations execute simultaneously for efficiency

**Version Storage:**
- Each website has 3 versions stored in `website_versions` table
- Tracks: version_number (1-3), generation_instructions, generated_html
- Users can switch between versions in real-time via the UI
- At least 1 successful version required for job completion

### API Endpoints:
- `GET /` - Main web interface with generator form and recent websites gallery
- `POST /generate` - Start async website generation (returns job_id)
- `GET /status/{job_id}` - Poll job status and get results
- `GET /websites` - Get 10 most recent generated websites with version metadata
- `GET /website/{identifier}` - View website with version switcher UI
- `GET /raw/{identifier}/{version_number}` - Serve raw HTML for iframe embedding (versions 1-3)
- `GET /health` - Service health check

### Generation Pipeline:
1. **Job Creation**: Create job record with `pending` status, return job_id to frontend
2. **Identifier Extraction**: Generate unique identifier from URL (with collision handling)
3. **Website Scraping**: Extract title, content, metadata, and images using BeautifulSoup
4. **Database Record**: Create website record with original HTML and scraped data
5. **Image Processing**: Convert all images to Cloudinary URLs and store mappings in database
6. **Creative Directions**: GPT-4-turbo generates 3 distinct design instructions (JSON output)
7. **Parallel Generation**: Launch 3 async tasks to generate HTML using GPT-5.1 (high reasoning)
8. **Version Storage**: Save each successful version to `website_versions` table
9. **Job Completion**: Mark job as `completed` if at least 1 version succeeded, otherwise `failed`
10. **Frontend Display**: User can view and switch between all generated versions

### Development:
```bash
# View logs:
docker-compose logs -f backend

# Database access:
docker-compose exec postgres psql -U postgres -d website_generator

# Add dependencies:
docker-compose run --rm --no-deps backend pip install <package>
docker-compose up --build -d

# Database tables:
# - websites: stores identifier, original_url, original_html, timestamps
# - website_versions: stores version_number (1-3), generation_instructions, generated_html per website
# - jobs: tracks async job status (pending, processing, completed, failed)
# - image_mappings: tracks original URLs to Cloudinary URL conversions
```